Installing the dependencies:
1. TailwindCSS for styling
2. PostCSS to manipulate CSS with JS
3. Autoprefixer to decides which properties and values to prefix by checking data from Can I Use, a website that tracks what web browsers support what

- Removing Home.module.css and cleaning globals.css for everything and adding @tailwind using folders

- Scrape all links from PG's site - Make a getLinks function

- Create datatypes to store and transfer essays, chunks i.e parts of essays to create embedding on, Json object that encodes PG's essay with chunks

- Scrape the essays in each link - Make a getEssay function

- Break all the essays into chunks of size 200 tokens and push into a datatype to create a chunked version of the entire essay to feed into gpt model

Connecting Supabase:
- Add the public url and service key in .env.local

TODO

- Go to SQL Editor and create a new query
    create or replace function pg_embsim_search(
        query_embeddings vector(1536),
        similary_threshold float,
        match_count int
    )
    returns table(
        id bigint,
        essay_title text,
        essay_url text,
        essay_date text,
        content text,
        content_tokens bigint,
        similarity float
    )
    language plpgsql
    as $$
    begin
    return query
    select 
        pgtalk_db.id,
        pgtalk_db.essay_title,
        pgtalk_db.essay_url,
        pgtalk_db.essay_date,
        pgtalk_db.content,
        pgtalk_db.content_tokens,
        1-(pgtalk_db.embeddings <=> query_embeddings) as similarity
    from
        pgtalk_db
    where 1-(pgtalk_db.embeddings <=> query_embeddings) > similarity_threshold
    order by pgtalk_db.embeddings <=> query_embeddings
    limit match_count;
    end;
    $$;

- Create and embed.ts in scripts and install : npm i @next/env -D

- Install openai : npm i openai -D
- Install supabase: npm i @supabase/supabase-js -D
< Add SupaBase Connection workflow>

- Install(not dev dependencies) eventsource-parser (for data streaming application) and endent(spacing and formatting utility)
- Create a Search.ts file in Pages/API to create embedding of query-> send to supabase for similarity search -> Get chunks that are similar to query (atleast 50% similar)
- Create a Answer.ts file in Pages/API to use Chat function of OpenAI API and stream the data that UI will render

- Explaining Encoding and Decoding flow need:
    When getting response for OpenAI APIs, we can get fragmented responses which is opposite to the world 'Stream', i.e a continuous body of data
    - To bypass this problem, we get the Server-Side Events containing the chunks of contents, which we capture continously and continously encode them to UTF-8 and put in the controller queue
        We then take these encoded chunks and dequeue the controller queue on by one and decode these chunks which we can stream to our UI